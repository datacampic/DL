{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN & DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN(Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인공신경망 : 사람의 신경망 원리와 구조를 모방하여 만든 기계학습 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://mblogthumb-phinf.pstatic.net/MjAxODAzMjZfMjg2/MDAxNTIyMDU4Nzk0Njgw.OFEKiuQ4i9H-9mtdtKRqux3K9QACykyCjdu8E32H3OUg.pgB4jU3DbbGARDAwe35ZUrbbshwZnEaqC4EKyRavcXwg.PNG.apr407/560px-Artificial_neural_network.svg.png?type=w800\" width =300 height = 200 align = 'left'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화 함수\n",
    "\n",
    "#### 1. sigmoid 함수 : Logistci함수라고도 하며, x의 값에 따라 0~1의 값을 출력하는 S자형 함수이다.\n",
    "#### <img src = 'https://mblogthumb-phinf.pstatic.net/MjAyMDAyMjVfNDQg/MDAxNTgyNjA2Mzc5MDY5.zxVCMRhPDevOiEqeW9Zld_qIExdLNovxjzMgD5csjKQg.U7vFTEIlTe8blGu1s6wFptli8_X1oe-QOfztJHWN7-og.PNG.handuelly/image.png?type=w800' width = 200 height = 150 align = 'left'></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. 하이퍼볼릭 탄젠트(Tanh) 함수 : 쌍곡선 함수중 하나로 -1과 1사이의 값으로 변환해주는 함수이다.\n",
    "#### <p><img src = 'https://mblogthumb-phinf.pstatic.net/MjAyMDAyMjVfMTk0/MDAxNTgyNjA3NzcyOTI3.Oc7diAw06G0vb-b86Wp5O-sy2Oa_bHHZnxnc2ASFYjAg.0dxRIz0WS3xkT51tlC8Yl1w_pDDRWynAaCX4NzxIq3Eg.PNG.handuelly/image.png?type=w800' width = 200 height = 150 align = 'left'></img></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ReLU 함수 : 최근 딥러닝에서 많이 사용하는 함수로 입력값이 0 이하면 0, 0보다 크면 x값을 그대로 반환한다. Sigmoid와 tanh가 같는 Gradient Vanishing(기울기 소실)문제를 해결한 함수이다.\n",
    "#### <p><img src = 'https://mblogthumb-phinf.pstatic.net/MjAyMDAyMjVfMTA4/MDAxNTgyNjA4MzM2NjI2.BimoIN4e0LyJoEdFhNfXO1q8o9FcMRzCZVnmBNrRqSog.o6c5C2zBc0Wh9YwxR37drT9VvZP_qE4yhSWRw2V68Dkg.PNG.handuelly/image.png?type=w800' width = 200 height = 150 align = 'left'></img></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Leaky ReLU 함수 : ReLU가 갖는 Dying ReLU(뉴런이 죽는 현상)을 해결하기 위해 나온 함수이다. Dying ReLU는 ReLU모델이 학습하는 동안 일부 뉴런이 0만을 출력하여 활성화 되지 않은 문제로, Leaky ReLU는 이를 해결하기 위해 입력값이 0보다 작은 경우 매우 작은 기울기를 부여하여 출력값이 0이 되는 것을 막는다.\n",
    "#### <p><img src = 'https://mblogthumb-phinf.pstatic.net/MjAyMDAyMjVfMjU5/MDAxNTgyNjA4NzA5MzUx.8bwP5NUnWan-vKq91HKuFL-FdZyG-nLVx-E2f03EMtEg.r09fBQqyqbI9-iSw8x2gla2TAuTBRpuEfBrlOyhiLMAg.PNG.handuelly/image.png?type=w800' width = 200 height = 150 align = 'left'></img></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Softmax 함수 : 다중 클래스 분류 문제에 주로 사용하는 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src = 'https://mblogthumb-phinf.pstatic.net/MjAyMDAyMjVfMTQ3/MDAxNTgyNjA5NDY3MTY3.228bUv_5mrol1w7X0NiFMD1UNru9zyf3yIJGcON-An0g.3Kzynlja9y_F9yTfANl937elQAK1pTGoJ_al7Om7TYsg.PNG.handuelly/image.png?type=w800' width = 400 height = 300 align = 'left'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 옵티마이저(Optimizer)\n",
    "\n",
    "#### <img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FskVjU%2Fbtq6AuBPHqG%2FMv8mirqP1DXFbkepoLsksK%2Fimg.png' width = 600 height = 450></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 확률적 경사 하강법(SGD)\n",
    "+ 매 스텝에서 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그레이디언트를 계산한다. 매 반복에서 하나의 샘플만 메모리에 있으면 되기 때문에 매우 큰 훈련세트를 훈련시키는 데 좋다. \n",
    "+ 하지만 확률적(무작위)이기 때문에 불안정하다. 무작위성은 지역 최솟값에서 탈출시켜줘서 좋지만 알고리즘을 전역 최솟값에 도달하지 못하게하여 좋지 않다. 이를 해결하는 방법으로는 학습률을 점진적으로 감소시키는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 모멘텀(Momentum)\n",
    "+ 지그재그로 움직이는 SGD를 개선하기 위해 등장한 옵티마이저로, 관성과 가속도를 넣어 지그재그가 아니라 한 방향으로 이동할 수 있게 개선한 방식이다. \n",
    "+ 현재 이동하는 방향과 별개로 이전에 이동했던 방향을 기억하여 과거의 방향을 일정량 추가하여 이동하게 된다. 하지만 과거의 행동을 저장해야하므로 메모리를 더 소모한다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. AdaGrad\n",
    "+ 학습률이 너무 작으면 학습시간이 길어지고, 너무 크면 발산해서 학습이 제대로 이루어 지지 않는다. AdaGrad는 학습률 감소로 이런 문제를 해결한다.\n",
    "+ AdaGrad는 이전 기울기의 제곱이 누적되어 더해지는데 무한히 학습을하다보면 이 값이 너무 커져서 학습이 되지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RMSProp\n",
    "+ 기울기를 단순 누적하지 않고 지수 가중 이동 평균을 사용하여 최신 기울기들이 더 크게 반영됨으로써 AdaGrad의 문제점을 해결한 방식이다. \n",
    "    - 지수 가중 이동 평균 : 최근 관측치에 비중을 더두면서 이동평균을 계산하는 방법\n",
    "+ 간단한 문제를 제외하고는 RMSProp이 항상 AdaGrad보다 훨씬 더 성능이 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Adam\n",
    "+ 모멘텀(Momentum)과 RmsProp의 방식을 합친 것이다.\n",
    "+ 모멘텀처럼 지난 기울기의 지수 감소 평균을 따르고 RMSProp처럼 지난 기울기 제곱의 지수 감소된 평균을 따른다.\n",
    "+ Adagrad나 RMSProp처럼 적응적 학습률 알고리즘이기 때문에 학습률 하이퍼파라미터를 튜닝할 필요가 없다.\n",
    "+ Adam을 쓰는것을 권장한다.\n",
    "+ Adam을 변형한 AdaMax와 Nadam도 존재한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퍼셉트론 : 초기 형태의 인공 신경망으로 다수의 입력으로 부터 하나의 출력을 만드는 알고리즘이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 단층 퍼셉트론 : 입력층과 출력층으로만 구성되어있는 퍼셉트론\n",
    "### <img src = \"https://wikidocs.net/images/page/24958/perceptron3_final.PNG\" align = 'left'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device == 'cude' :\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0,0], [0,1], [1,0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(2, 1, bias=True)\n",
    "sigmoid = nn.Sigmoid()\n",
    "model = nn.Sequential(linear, sigmoid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss().to(device) # 이진분류에서 사용하는 크로스 엔트로피 함수\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7273974418640137\n",
      "100 0.6931476593017578\n",
      "200 0.6931471824645996\n",
      "300 0.6931471824645996\n",
      "400 0.6931471824645996\n",
      "500 0.6931471824645996\n",
      "600 0.6931471824645996\n",
      "700 0.6931471824645996\n",
      "800 0.6931471824645996\n",
      "900 0.6931471824645996\n",
      "1000 0.6931471824645996\n",
      "1100 0.6931471824645996\n",
      "1200 0.6931471824645996\n",
      "1300 0.6931471824645996\n",
      "1400 0.6931471824645996\n",
      "1500 0.6931471824645996\n",
      "1600 0.6931471824645996\n",
      "1700 0.6931471824645996\n",
      "1800 0.6931471824645996\n",
      "1900 0.6931471824645996\n",
      "2000 0.6931471824645996\n",
      "2100 0.6931471824645996\n",
      "2200 0.6931471824645996\n",
      "2300 0.6931471824645996\n",
      "2400 0.6931471824645996\n",
      "2500 0.6931471824645996\n",
      "2600 0.6931471824645996\n",
      "2700 0.6931471824645996\n",
      "2800 0.6931471824645996\n",
      "2900 0.6931471824645996\n",
      "3000 0.6931471824645996\n",
      "3100 0.6931471824645996\n",
      "3200 0.6931471824645996\n",
      "3300 0.6931471824645996\n",
      "3400 0.6931471824645996\n",
      "3500 0.6931471824645996\n",
      "3600 0.6931471824645996\n",
      "3700 0.6931471824645996\n",
      "3800 0.6931471824645996\n",
      "3900 0.6931471824645996\n",
      "4000 0.6931471824645996\n",
      "4100 0.6931471824645996\n",
      "4200 0.6931471824645996\n",
      "4300 0.6931471824645996\n",
      "4400 0.6931471824645996\n",
      "4500 0.6931471824645996\n",
      "4600 0.6931471824645996\n",
      "4700 0.6931471824645996\n",
      "4800 0.6931471824645996\n",
      "4900 0.6931471824645996\n",
      "5000 0.6931471824645996\n",
      "5100 0.6931471824645996\n",
      "5200 0.6931471824645996\n",
      "5300 0.6931471824645996\n",
      "5400 0.6931471824645996\n",
      "5500 0.6931471824645996\n",
      "5600 0.6931471824645996\n",
      "5700 0.6931471824645996\n",
      "5800 0.6931471824645996\n",
      "5900 0.6931471824645996\n",
      "6000 0.6931471824645996\n",
      "6100 0.6931471824645996\n",
      "6200 0.6931471824645996\n",
      "6300 0.6931471824645996\n",
      "6400 0.6931471824645996\n",
      "6500 0.6931471824645996\n",
      "6600 0.6931471824645996\n",
      "6700 0.6931471824645996\n",
      "6800 0.6931471824645996\n",
      "6900 0.6931471824645996\n",
      "7000 0.6931471824645996\n",
      "7100 0.6931471824645996\n",
      "7200 0.6931471824645996\n",
      "7300 0.6931471824645996\n",
      "7400 0.6931471824645996\n",
      "7500 0.6931471824645996\n",
      "7600 0.6931471824645996\n",
      "7700 0.6931471824645996\n",
      "7800 0.6931471824645996\n",
      "7900 0.6931471824645996\n",
      "8000 0.6931471824645996\n",
      "8100 0.6931471824645996\n",
      "8200 0.6931471824645996\n",
      "8300 0.6931471824645996\n",
      "8400 0.6931471824645996\n",
      "8500 0.6931471824645996\n",
      "8600 0.6931471824645996\n",
      "8700 0.6931471824645996\n",
      "8800 0.6931471824645996\n",
      "8900 0.6931471824645996\n",
      "9000 0.6931471824645996\n",
      "9100 0.6931471824645996\n",
      "9200 0.6931471824645996\n",
      "9300 0.6931471824645996\n",
      "9400 0.6931471824645996\n",
      "9500 0.6931471824645996\n",
      "9600 0.6931471824645996\n",
      "9700 0.6931471824645996\n",
      "9800 0.6931471824645996\n",
      "9900 0.6931471824645996\n",
      "10000 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001): \n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0: # 100번째 에포크마다 비용 출력\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200번째 에포크 뒤로 비용함수 값이 같으므로 단층 퍼셉트론은 XOR 문제를 해결할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis):  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "모델의 예측값(Predicted):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "실제값(Y):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도(Accuracy):  0.5\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())\n",
    "    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())\n",
    "    print('실제값(Y): ', Y.cpu().numpy())\n",
    "    print('정확도(Accuracy): ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실제값과 예측값이 다르게 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 다층 퍼셉트론 : 입력층과 출력층 사이에 중간층을 더 추가하였다. 이 입력층과 출력층 사이에 존재하는 중간층을 은닉층(hidden layer)라고 한다.\n",
    "### <img src = \"https://wikidocs.net/images/page/24958/perceptron_4image.jpg\" align = 'left'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "          nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1\n",
    "          nn.Sigmoid()\n",
    "          ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/61010/ann.PNG\" align = 'left' width = 300 height = 200></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6940630078315735\n",
      "100 0.6931129097938538\n",
      "200 0.6931090354919434\n",
      "300 0.6931048035621643\n",
      "400 0.6930999755859375\n",
      "500 0.6930946707725525\n",
      "600 0.6930886507034302\n",
      "700 0.6930818557739258\n",
      "800 0.6930739879608154\n",
      "900 0.6930649876594543\n",
      "1000 0.6930544376373291\n",
      "1100 0.6930420398712158\n",
      "1200 0.6930272579193115\n",
      "1300 0.6930094361305237\n",
      "1400 0.6929875016212463\n",
      "1500 0.6929603219032288\n",
      "1600 0.6929258108139038\n",
      "1700 0.6928810477256775\n",
      "1800 0.6928213834762573\n",
      "1900 0.6927394866943359\n",
      "2000 0.6926223039627075\n",
      "2100 0.6924462914466858\n",
      "2200 0.6921650171279907\n",
      "2300 0.6916765570640564\n",
      "2400 0.6907237768173218\n",
      "2500 0.6885190010070801\n",
      "2600 0.6817606091499329\n",
      "2700 0.6487735509872437\n",
      "2800 0.5389856100082397\n",
      "2900 0.345180481672287\n",
      "3000 0.01767934113740921\n",
      "3100 0.007422953844070435\n",
      "3200 0.004489727783948183\n",
      "3300 0.0031565302051603794\n",
      "3400 0.0024085163604468107\n",
      "3500 0.0019345718901604414\n",
      "3600 0.0016094421735033393\n",
      "3700 0.0013736033579334617\n",
      "3800 0.001195303164422512\n",
      "3900 0.001056042267009616\n",
      "4000 0.0009444837924093008\n",
      "4100 0.0008533399668522179\n",
      "4200 0.0007774304831400514\n",
      "4300 0.0007133669569157064\n",
      "4400 0.0006585969822481275\n",
      "4500 0.0006113001145422459\n",
      "4600 0.0005700589390471578\n",
      "4700 0.0005337543552741408\n",
      "4800 0.0005016404902562499\n",
      "4900 0.0004730012733489275\n",
      "5000 0.0004472997388802469\n",
      "5100 0.0004241629794705659\n",
      "5200 0.0004032031574752182\n",
      "5300 0.0003840922727249563\n",
      "5400 0.0003666662087198347\n",
      "5500 0.00035068640136159956\n",
      "5600 0.00033597383298911154\n",
      "5700 0.0003223794628866017\n",
      "5800 0.00030979886651039124\n",
      "5900 0.0002981425786856562\n",
      "6000 0.0002872913610190153\n",
      "6100 0.0002771705621853471\n",
      "6200 0.00026773553690873086\n",
      "6300 0.00025885208742693067\n",
      "6400 0.00025052018463611603\n",
      "6500 0.00024271002621389925\n",
      "6600 0.0002353321760892868\n",
      "6700 0.00022835680283606052\n",
      "6800 0.00022182860993780196\n",
      "6900 0.00021556873980443925\n",
      "7000 0.0002096964162774384\n",
      "7100 0.0002041073312284425\n",
      "7200 0.00019877160957548767\n",
      "7300 0.0001937191264005378\n",
      "7400 0.0001889051345642656\n",
      "7500 0.00018431470380164683\n",
      "7600 0.0001799478632165119\n",
      "7700 0.000175744979060255\n",
      "7800 0.00017172095249406993\n",
      "7900 0.00016790561494417489\n",
      "8000 0.00016426912043243647\n",
      "8100 0.00016075186431407928\n",
      "8200 0.00015735384658910334\n",
      "8300 0.00015414960216730833\n",
      "8400 0.00015097516006790102\n",
      "8500 0.0001479944767197594\n",
      "8600 0.00014513303176499903\n",
      "8700 0.00014233120600692928\n",
      "8800 0.00013964861864224076\n",
      "8900 0.0001370703539578244\n",
      "9000 0.0001345517230220139\n",
      "9100 0.00013215230137575418\n",
      "9200 0.00012982741463929415\n",
      "9300 0.00012756214709952474\n",
      "9400 0.0001254012022400275\n",
      "9500 0.000123284975416027\n",
      "9600 0.00012125817011110485\n",
      "9700 0.00011926117440452799\n",
      "9800 0.00011736848682630807\n",
      "9900 0.00011549072223715484\n",
      "10000 0.00011370238644303754\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    # forward 연산\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100의 배수에 해당되는 에포크마다 비용을 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis):  [[9.54754432e-05]\n",
      " [9.99877214e-01]\n",
      " [9.99881506e-01]\n",
      " [1.17857926e-04]]\n",
      "모델의 예측값(Predicted):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "실제값(Y):  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도(Accuracy):  1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())\n",
    "    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())\n",
    "    print('실제값(Y): ', Y.cpu().numpy())\n",
    "    print('정확도(Accuracy): ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층퍼셉트론으로 MNIST 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.target = mnist.target.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data / 255  # 0-255값을 [0,1] 구간으로 정규화\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
       "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
       "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
       "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
       "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
       "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
       "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
       "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
       "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
       "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
       "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
       "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
       "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 이미지 데이터의 레이블은 5이다\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].reshape(28, 28), cmap='gray')\n",
    "print(\"이 이미지 데이터의 레이블은 {:.0f}이다\".format(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=0)\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('fc1', nn.Linear(28*28*1, 100))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('fc2', nn.Linear(100, 100))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('fc3', nn.Linear(100, 10))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# 오차함수 선택\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 가중치를 학습하기 위한 최적화 기법 선택\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()  # 신경망을 학습 모드로 전환\n",
    "\n",
    "    # 데이터로더에서 미니배치를 하나씩 꺼내 학습을 수행\n",
    "    for data, targets in loader_train:\n",
    "\n",
    "        optimizer.zero_grad()  # 경사를 0으로 초기화\n",
    "        outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "        loss = loss_fn(outputs, targets)  # 출력과 훈련 데이터 정답 간의 오차를 계산\n",
    "        loss.backward()  # 오차를 역전파 계산\n",
    "        optimizer.step()  # 역전파 계산한 값으로 가중치를 수정\n",
    "\n",
    "    print(\"epoch{}：완료\\n\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # 신경망을 추론 모드로 전환\n",
    "    correct = 0\n",
    "\n",
    "    # 데이터로더에서 미니배치를 하나씩 꺼내 추론을 수행\n",
    "    with torch.no_grad():  # 추론 과정에는 미분이 필요없음\n",
    "        for data, targets in loader_test:\n",
    "\n",
    "            outputs = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "\n",
    "            # 추론 계산\n",
    "            _, predicted = torch.max(outputs.data, 1)  # 확률이 가장 높은 레이블이 무엇인지 계산\n",
    "            correct += predicted.eq(targets.data.view_as(predicted)).sum()  # 정답과 일치한 경우 정답 카운트를 증가\n",
    "\n",
    "    # 정확도 출력\n",
    "    data_num = len(loader_test.dataset)  # 데이터 총 건수\n",
    "    print('\\n테스트 데이터에서 예측 정확도: {}/{} ({:.0f}%)\\n'.format(correct,\n",
    "                                                   data_num, 100. * correct / data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 데이터에서 예측 정확도: 1042/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0：완료\n",
      "\n",
      "epoch1：완료\n",
      "\n",
      "epoch2：완료\n",
      "\n",
      "\n",
      "테스트 데이터에서 예측 정확도: 9574/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과 : 2\n",
      "이 이미지 데이터의 정답 레이블은 2입니다\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 2018\n",
    "\n",
    "model.eval()  # 신경망을 추론 모드로 전환\n",
    "data = X_test[index]\n",
    "output = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "_, predicted = torch.max(output.data, 0)  # 확률이 가장 높은 레이블이 무엇인지 계산\n",
    "\n",
    "print(\"예측 결과 : {}\".format(predicted))\n",
    "\n",
    "X_test_show = (X_test[index]).numpy()\n",
    "plt.imshow(X_test_show.reshape(28, 28), cmap='gray')\n",
    "print(\"이 이미지 데이터의 정답 레이블은 {:.0f}입니다\".format(y_test[index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN(Deep Neural Network)\n",
    "\n",
    "### 심층 신경망(DNN) : 은닉층이 2개 이상인 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model done\n"
     ]
    }
   ],
   "source": [
    "#Define Neural Networks Model.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        h1 = F.relu(self.fc1(x.view(-1, 784)))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        h3 = F.relu(self.fc3(h2))\n",
    "        h4 = F.relu(self.fc4(h3))\n",
    "        h5 = F.relu(self.fc5(h4))\n",
    "        h6 = self.fc6(h5)\n",
    "        return F.log_softmax(h6, dim=1)\n",
    "\n",
    "print(\"init model done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set vars and device done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # 몇개의 샘플로 가중치를 갱신할 것인지 설정\n",
    "test_batch_size = 1000\n",
    "epochs = 10 # 전체 데이터 셋을 몇 번 반복할지 설정\n",
    "lr = 0.01 # 학습률\n",
    "momentum = 0.5 #momentum 계수 지정\n",
    "no_cuda = True # GPU 사용 유무 체크?\n",
    "seed = 1 \n",
    "# pytorch는 progress bar가 돌아가지 않아서 직접 출력을 통해 확인하기 위한 학습 로그 interval을 생성...한다고 되있지만 이해x\n",
    "log_interval = 200\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "print(\"set vars and device done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e152c5ea71644afc9c798d3314488b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd62857c74b34bbe833b331fb846c1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dec65e977e7450393c4642d997feaed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e63148b5064d84905b075f91765ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  datasets.MNIST('../data', train=True, download=True, \n",
    "                 transform=transform), \n",
    "    batch_size = batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "                 transform=transform), \n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# SGD를 모멘텀으로 변형한 것. -> 관성을 주게되면서 지역 최소값에 도달하더라도 앞으로 나아가서 지역 최소값을 탈출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Train function and Test function to validate.\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(log_interval, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format\n",
    "          (test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290312\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.296319\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.270589\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.144817\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.135795\n",
      "\n",
      "Test set: Average loss: 0.7310, Accuracy: 7531/10000 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.834217\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.455744\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.358269\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.221012\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.497491\n",
      "\n",
      "Test set: Average loss: 0.3307, Accuracy: 9005/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.695523\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.092797\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.262283\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.203517\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.204129\n",
      "\n",
      "Test set: Average loss: 0.1757, Accuracy: 9500/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.153977\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.115303\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.115208\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.043359\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.238541\n",
      "\n",
      "Test set: Average loss: 0.1322, Accuracy: 9588/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.050499\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.036636\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.055252\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.142012\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.026438\n",
      "\n",
      "Test set: Average loss: 0.1130, Accuracy: 9655/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.024776\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.116158\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.056980\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.039198\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.064553\n",
      "\n",
      "Test set: Average loss: 0.0987, Accuracy: 9708/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.030815\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.035516\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.023841\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.158986\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.061319\n",
      "\n",
      "Test set: Average loss: 0.0936, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.056695\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.005032\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.016507\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.055038\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.038493\n",
      "\n",
      "Test set: Average loss: 0.0914, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.015031\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.041476\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.038432\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.006870\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.060995\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.017471\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.055950\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.016832\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.054092\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.004703\n",
      "\n",
      "Test set: Average loss: 0.1035, Accuracy: 9709/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and Test the model and save it.\n",
    "for epoch in range(1, 11):\n",
    "    train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    test(log_interval, model, device, test_loader)\n",
    "torch.save(model, './model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
